{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "- 탐험(Exploration)을 하면서도 최적의 정책을 학습이 가능함(상대적으로 SARSA에 비해서)\n",
    "- Off-Policy\n",
    "    - SARSA와 같은 on-policy는 이전 정책으로부터 얻은 샘플을 재사용할 수 없고, 자신의 샘플로 자신을 업데이트하므로 문제점이 존재할 수 있음\n",
    "    - 정책이 복수개이므로 behavior policy(현재의 예시에선 입실론 그리디)로 샘플을 수집하고 target policy로 최적의 정책을 학습\n",
    "- 학습하는 정책(TD) : 탐욕정책 -> exploitation\n",
    "\n",
    "$$q(s, a) = q(s, a) + \\alpha \\big( r + \\gamma\\ max_{a'} q(s', a') - q(s, a)  \\big )$$\n",
    "\n",
    "- 행동하는 정책 : $\\epsilon$-greedy -> exploration\n",
    "    - 종류 : $\\epsilon$-greedy, BoltMann, Bayesian, ...\n",
    "    - 아래의 예는 SARSA와 같은 입실론 그리디\n",
    "    \n",
    "$$\\pi(s) = \\begin{cases} \n",
    "    a^* = argmax_{a \\in A} q(s, a), 1 - \\epsilon \\\\\n",
    "    \\text{random action}, \\ \\epsilon \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning의 학습 순서\n",
    "1. 상태 s에서 행동하는 a는 행동 정책(입실론탐욕)으로 선택\n",
    "2. 환경으로부터 다음 상태 s'와 보상 r을 받음\n",
    "3. 벨만 최적 방정식을 통해 q(s, a)를 업데이트\n",
    "    - 학습하는 정책 : 탐욕정책\n",
    "\n",
    "$$q(s, a) = q(s, a) + \\alpha \\big( r + \\gamma\\ max_{a'} q(s', a') - q(s, a)  \\big )$$\n",
    "\n",
    "- 벨만 최적 방정식을 이용하므로 s'에서 a'를 선택하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T05:53:53.056121Z",
     "start_time": "2018-05-07T05:53:51.537101Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from environment import Env\n",
    "from collections import defaultdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T05:53:53.148884Z",
     "start_time": "2018-05-07T05:53:53.093783Z"
    }
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, actions):\n",
    "        # 행동 = [0, 1, 2, 3] 순서대로 상, 하, 좌, 우\n",
    "        self.actions = actions\n",
    "        self.learnging_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "        \n",
    "    # <s, a, r, s'> 샘플로부터 큐함수 업데이트(exploitation), greedy\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        q_1 = self.q_table[state][action]\n",
    "        # 벨만 최적 방정식 \n",
    "        q_2 = reward + self.discount_factor * max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.learnging_rate * (q_2 - q_1)\n",
    "        \n",
    "    # 큐함수로 입실론 탐욕 정책에 따라서 행동을 반환한다\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # 큐함수에 따른 행동 반환\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "    \n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent에 따른 정책 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T06:20:32.524074Z",
     "start_time": "2018-05-07T05:53:55.839246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  0  y : 0 [0.0, 1.9474449511909893e-06, 0.0, 0.0]\n",
      "x :  1  y : 0 [0.0, 0.0, 1.2461694768832958e-08, 0.0]\n",
      "x :  2  y : 0 [0.0, -1.0, 0.0, 0.0]\n",
      "x :  3  y : 0 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  4  y : 0 [0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "x :  0  y : 1 [0.0, 8.392766731440504e-05, 0.0, 1.5836941800000003e-12]\n",
      "x :  1  y : 1 [0.0, -1.0, 1.0464132339e-09, -2.9701]\n",
      "x :  2  y : 1 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y : 1 [0.0, 0.009000000000000001, -1.0, 0.0]\n",
      "x :  4  y : 1 [0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "x :  0  y : 2 [0.0, 0.0028435541426892566, 1.9979487303020747e-06, -1.99]\n",
      "x :  1  y : 2 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  2  y : 2 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y : 2 [0.0, 0.0, 1.99, 0.0]\n",
      "x :  4  y : 2 [0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "x :  0  y : 3 [0.0, 0.0, 0.00048380020214232284, 0.0782519567627358]\n",
      "x :  1  y : 3 [-1.0, 7.290000000000002e-07, 0.0, 1.6363086690409852]\n",
      "x :  2  y : 3 [18.20930624027691, 0.0, 0.009970278279254853, 0.0]\n",
      "x :  3  y : 3 [0.0, 0.0, 0.12594748082284032, 0.0]\n",
      "x :  4  y : 3 [0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "x :  0  y : 4 [0.0, 0.0, 0.0, 3.848185050439681e-05]\n",
      "x :  1  y : 4 [0.01103801120452825, 0.0, 0.0, 0.0]\n",
      "x :  2  y : 4 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y : 4 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  4  y : 4 [0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 1000\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = QLearningAgent(actions=list(range(env.n_actions)))\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            env.render()\n",
    "            \n",
    "            # 현재 상태에 대한 행동을 선택\n",
    "            action = agent.get_action(str(state))\n",
    "            \n",
    "            # 행동을 취한 후 다음 상태, 보상, 에피소드의 종료 여부를 받아온다\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # <s, a, r, s'>로 큐함수를 업데이트\n",
    "            agent.learn(str(state), action, reward, str(next_state))\n",
    "            state = next_state\n",
    "            \n",
    "            # 모든 큐함수를 화면에 표시\n",
    "            env.print_value_all(agent.q_table)\n",
    "            \n",
    "            if done:\n",
    "                if episode == 30:\n",
    "                    for y in range(5):\n",
    "                        for x in range(5):\n",
    "                            print(\"x : \", x, ' y :', y, agent.q_table[str([x, y])])\n",
    "                        print(\"\")\n",
    "                    time.sleep(100)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
