{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "- 탐험(Exploration)을 하면서도 최적의 정책을 학습이 가능함(상대적으로 SARSA에 비해서)\n",
    "- Off-Policy\n",
    "    - SARSA와 같은 on-policy는 이전 정책으로부터 얻은 샘플을 재사용할 수 없고, 자신의 샘플로 자신을 업데이트하므로 문제점이 존재할 수 있음\n",
    "    - 정책이 복수개이므로 behavior policy(현재의 예시에선 입실론 그리디)로 샘플을 수집하고 target policy로 최적의 정책을 학습\n",
    "- 학습하는 정책(TD) : 탐욕정책 -> exploitation\n",
    "\n",
    "$$q(s, a) = q(s, a) + \\alpha \\big( r + \\gamma\\ max_{a'} q(s', a') - q(s, a)  \\big )$$\n",
    "\n",
    "- 행동하는 정책 : $\\epsilon$-greedy -> exploration\n",
    "    - 종류 : $\\epsilon$-greedy, BoltMann, Bayesian, ...\n",
    "    - 아래의 예는 SARSA와 같은 입실론 그리디\n",
    "    \n",
    "$$\\pi(s) = \\begin{cases} \n",
    "    a^* = argmax_{a \\in A} q(s, a), 1 - \\epsilon \\\\\n",
    "    \\text{random action}, \\ \\epsilon \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T06:24:24.018494Z",
     "start_time": "2018-05-07T06:24:23.720847Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import ylim\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import gym\n",
    "import gym_maze\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T06:24:48.205279Z",
     "start_time": "2018-05-07T06:24:24.235036Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-07 15:24:24,745] Making new env: maze-sample-10x10-v0\n",
      "/home/paulkim/anaconda3/envs/reinforce_learning0101/lib/python3.5/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/paulkim/anaconda3/envs/reinforce_learning0101/lib/python3.5/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 9), (0, 9)]\n",
      "Episode : 0 total reward = -3.510000 .\n",
      "Episode : 1 total reward = -4.667000 .\n",
      "Episode : 2 total reward = -1.321000 .\n",
      "Episode : 3 total reward = -2.862000 .\n",
      "Episode : 4 total reward = -4.743000 .\n",
      "Episode : 5 total reward = 0.184000 .\n",
      "Episode : 6 total reward = -0.275000 .\n",
      "Episode : 7 total reward = 0.157000 .\n",
      "Episode : 8 total reward = -1.425000 .\n",
      "Episode : 9 total reward = -0.368000 .\n",
      "Episode : 10 total reward = -0.192000 .\n",
      "Episode : 11 total reward = -1.051000 .\n",
      "Episode : 12 total reward = -1.129000 .\n",
      "Episode : 13 total reward = 0.460000 .\n",
      "Episode : 14 total reward = -0.067000 .\n",
      "Episode : 15 total reward = -0.244000 .\n",
      "Episode : 16 total reward = -0.095000 .\n",
      "Episode : 17 total reward = -0.092000 .\n",
      "Episode : 18 total reward = -0.112000 .\n",
      "Episode : 19 total reward = 0.338000 .\n",
      "Episode : 20 total reward = -0.186000 .\n",
      "Episode : 21 total reward = -0.177000 .\n",
      "Episode : 22 total reward = 0.062000 .\n",
      "Episode : 23 total reward = -0.977000 .\n",
      "Episode : 24 total reward = -0.176000 .\n",
      "Episode : 25 total reward = 0.627000 .\n",
      "Episode : 26 total reward = 0.229000 .\n",
      "Episode : 27 total reward = 0.190000 .\n",
      "Episode : 28 total reward = 0.371000 .\n",
      "Episode : 29 total reward = 0.334000 .\n",
      "Episode : 30 total reward = -0.143000 .\n",
      "Episode : 31 total reward = -0.121000 .\n",
      "Episode : 32 total reward = 0.566000 .\n",
      "Episode : 33 total reward = -0.108000 .\n",
      "Episode : 34 total reward = 0.221000 .\n",
      "Episode : 35 total reward = 0.338000 .\n",
      "Episode : 36 total reward = -0.234000 .\n",
      "Episode : 37 total reward = 0.253000 .\n",
      "Episode : 38 total reward = 0.379000 .\n",
      "Episode : 39 total reward = 0.104000 .\n",
      "Episode : 40 total reward = 0.309000 .\n",
      "Episode : 41 total reward = 0.216000 .\n",
      "Episode : 42 total reward = 0.597000 .\n",
      "Episode : 43 total reward = 0.285000 .\n",
      "Episode : 44 total reward = 0.469000 .\n",
      "Episode : 45 total reward = 0.005000 .\n",
      "Episode : 46 total reward = 0.420000 .\n",
      "Episode : 47 total reward = 0.120000 .\n",
      "Episode : 48 total reward = 0.145000 .\n",
      "Episode : 49 total reward = 0.276000 .\n",
      "Episode : 50 total reward = 0.107000 .\n",
      "Episode : 51 total reward = 0.100000 .\n",
      "Episode : 52 total reward = 0.465000 .\n",
      "Episode : 53 total reward = 0.219000 .\n",
      "Episode : 54 total reward = 0.016000 .\n",
      "Episode : 55 total reward = 0.375000 .\n",
      "Episode : 56 total reward = 0.438000 .\n",
      "Episode : 57 total reward = -0.097000 .\n",
      "Episode : 58 total reward = 0.430000 .\n",
      "Episode : 59 total reward = 0.024000 .\n",
      "Episode : 60 total reward = -0.043000 .\n",
      "Episode : 61 total reward = 0.424000 .\n",
      "Episode : 62 total reward = 0.293000 .\n",
      "Episode : 63 total reward = 0.267000 .\n",
      "Episode : 64 total reward = 0.493000 .\n",
      "Episode : 65 total reward = 0.384000 .\n",
      "Episode : 66 total reward = 0.425000 .\n",
      "Episode : 67 total reward = 0.376000 .\n",
      "Episode : 68 total reward = 0.015000 .\n",
      "Episode : 69 total reward = 0.102000 .\n",
      "Episode : 70 total reward = 0.450000 .\n",
      "Episode : 71 total reward = -0.093000 .\n",
      "Episode : 72 total reward = 0.323000 .\n",
      "Episode : 73 total reward = 0.171000 .\n",
      "Episode : 74 total reward = 0.404000 .\n",
      "Episode : 75 total reward = 0.021000 .\n",
      "Episode : 76 total reward = 0.183000 .\n",
      "Episode : 77 total reward = 0.434000 .\n",
      "Episode : 78 total reward = -0.067000 .\n",
      "Episode : 79 total reward = -0.055000 .\n",
      "Episode : 80 total reward = 0.062000 .\n",
      "Episode : 81 total reward = 0.545000 .\n",
      "Episode : 82 total reward = 0.452000 .\n",
      "Episode : 83 total reward = 0.071000 .\n",
      "Episode : 84 total reward = 0.255000 .\n",
      "Episode : 85 total reward = 0.162000 .\n",
      "Episode : 86 total reward = 0.336000 .\n",
      "Episode : 87 total reward = 0.344000 .\n",
      "Episode : 88 total reward = 0.535000 .\n",
      "Episode : 89 total reward = 0.210000 .\n",
      "Episode : 90 total reward = 0.103000 .\n",
      "Episode : 91 total reward = 0.156000 .\n",
      "Episode : 92 total reward = 0.401000 .\n",
      "Episode : 93 total reward = 0.202000 .\n",
      "Episode : 94 total reward = 0.662000 .\n",
      "Episode : 95 total reward = 0.507000 .\n",
      "Episode : 96 total reward = 0.211000 .\n",
      "Episode : 97 total reward = 0.258000 .\n",
      "Episode : 98 total reward = 0.380000 .\n",
      "Episode : 99 total reward = 0.099000 .\n",
      "Episode : 100 total reward = 0.343000 .\n",
      "Episode : 101 total reward = 0.146000 .\n",
      "Episode : 102 total reward = 0.537000 .\n",
      "Episode : 103 total reward = 0.224000 .\n",
      "Episode : 104 total reward = 0.334000 .\n",
      "Episode : 105 total reward = 0.457000 .\n",
      "Episode : 106 total reward = 0.452000 .\n",
      "Episode : 107 total reward = 0.375000 .\n",
      "Episode : 108 total reward = 0.079000 .\n",
      "Episode : 109 total reward = 0.498000 .\n",
      "Episode : 110 total reward = -0.126000 .\n",
      "Episode : 111 total reward = 0.119000 .\n",
      "Episode : 112 total reward = 0.275000 .\n",
      "Episode : 113 total reward = 0.420000 .\n",
      "Episode : 114 total reward = 0.036000 .\n",
      "Episode : 115 total reward = 0.405000 .\n",
      "Episode : 116 total reward = 0.140000 .\n",
      "Episode : 117 total reward = 0.065000 .\n",
      "Episode : 118 total reward = 0.180000 .\n",
      "Episode : 119 total reward = 0.470000 .\n",
      "Episode : 120 total reward = 0.204000 .\n",
      "Episode : 121 total reward = 0.335000 .\n",
      "Episode : 122 total reward = 0.445000 .\n",
      "Episode : 123 total reward = -0.182000 .\n",
      "Episode : 124 total reward = 0.507000 .\n",
      "Episode : 125 total reward = 0.168000 .\n",
      "Episode : 126 total reward = 0.163000 .\n",
      "Episode : 127 total reward = 0.064000 .\n",
      "Episode : 128 total reward = -0.208000 .\n",
      "Episode : 129 total reward = 0.583000 .\n",
      "Episode : 130 total reward = 0.211000 .\n",
      "Episode : 131 total reward = -0.283000 .\n",
      "Episode : 132 total reward = 0.259000 .\n",
      "Episode : 133 total reward = 0.199000 .\n",
      "Episode : 134 total reward = 0.399000 .\n",
      "Episode : 135 total reward = 0.356000 .\n",
      "Episode : 136 total reward = 0.442000 .\n",
      "Episode : 137 total reward = 0.463000 .\n",
      "Episode : 138 total reward = 0.236000 .\n",
      "Episode : 139 total reward = 0.349000 .\n",
      "Episode : 140 total reward = 0.381000 .\n",
      "Episode : 141 total reward = 0.073000 .\n",
      "Episode : 142 total reward = 0.217000 .\n",
      "Episode : 143 total reward = -0.110000 .\n",
      "Episode : 144 total reward = 0.193000 .\n",
      "Episode : 145 total reward = 0.214000 .\n",
      "Episode : 146 total reward = 0.623000 .\n",
      "Episode : 147 total reward = 0.142000 .\n",
      "Episode : 148 total reward = 0.619000 .\n",
      "Episode : 149 total reward = 0.280000 .\n",
      "Episode : 150 total reward = 0.434000 .\n",
      "Episode : 151 total reward = 0.631000 .\n",
      "Episode : 152 total reward = 0.566000 .\n",
      "Episode : 153 total reward = 0.138000 .\n",
      "Episode : 154 total reward = 0.205000 .\n",
      "Episode : 155 total reward = 0.317000 .\n",
      "Episode : 156 total reward = 0.632000 .\n",
      "Episode : 157 total reward = 0.624000 .\n",
      "Episode : 158 total reward = 0.601000 .\n",
      "Episode : 159 total reward = 0.347000 .\n",
      "Episode : 160 total reward = -0.062000 .\n",
      "Episode : 161 total reward = 0.453000 .\n",
      "Episode : 162 total reward = 0.267000 .\n",
      "Episode : 163 total reward = 0.262000 .\n",
      "Episode : 164 total reward = 0.447000 .\n",
      "Episode : 165 total reward = 0.385000 .\n",
      "Episode : 166 total reward = 0.032000 .\n",
      "Episode : 167 total reward = 0.032000 .\n",
      "Episode : 168 total reward = 0.522000 .\n",
      "Episode : 169 total reward = 0.432000 .\n",
      "Episode : 170 total reward = 0.294000 .\n",
      "Episode : 171 total reward = 0.417000 .\n",
      "Episode : 172 total reward = 0.313000 .\n",
      "Episode : 173 total reward = 0.631000 .\n",
      "Episode : 174 total reward = 0.385000 .\n",
      "Episode : 175 total reward = 0.010000 .\n",
      "Episode : 176 total reward = 0.192000 .\n",
      "Episode : 177 total reward = 0.504000 .\n",
      "Episode : 178 total reward = -0.128000 .\n",
      "Episode : 179 total reward = 0.183000 .\n",
      "Episode : 180 total reward = 0.307000 .\n",
      "Episode : 181 total reward = 0.601000 .\n",
      "Episode : 182 total reward = 0.593000 .\n",
      "Episode : 183 total reward = 0.615000 .\n",
      "Episode : 184 total reward = 0.374000 .\n",
      "Episode : 185 total reward = 0.365000 .\n",
      "Episode : 186 total reward = 0.243000 .\n",
      "Episode : 187 total reward = 0.531000 .\n",
      "Episode : 188 total reward = 0.063000 .\n",
      "Episode : 189 total reward = 0.369000 .\n",
      "Episode : 190 total reward = 0.513000 .\n",
      "Episode : 191 total reward = 0.184000 .\n",
      "Episode : 192 total reward = 0.355000 .\n",
      "Episode : 193 total reward = -0.198000 .\n",
      "Episode : 194 total reward = 0.011000 .\n",
      "Episode : 195 total reward = 0.184000 .\n",
      "Episode : 196 total reward = 0.447000 .\n",
      "Episode : 197 total reward = 0.585000 .\n",
      "Episode : 198 total reward = 0.439000 .\n",
      "Episode : 199 total reward = 0.242000 .\n",
      "Episode : 200 total reward = 0.487000 .\n",
      "Episode : 201 total reward = 0.276000 .\n",
      "Episode : 202 total reward = 0.134000 .\n",
      "Episode : 203 total reward = -0.011000 .\n",
      "Episode : 204 total reward = 0.261000 .\n",
      "Episode : 205 total reward = 0.368000 .\n",
      "Episode : 206 total reward = 0.245000 .\n",
      "Episode : 207 total reward = 0.582000 .\n",
      "Episode : 208 total reward = 0.388000 .\n",
      "Episode : 209 total reward = 0.365000 .\n",
      "Episode : 210 total reward = 0.592000 .\n",
      "Episode : 211 total reward = 0.356000 .\n",
      "Episode : 212 total reward = 0.674000 .\n",
      "Episode : 213 total reward = 0.442000 .\n",
      "Episode : 214 total reward = 0.349000 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 215 total reward = 0.333000 .\n",
      "Episode : 216 total reward = 0.688000 .\n",
      "Episode : 217 total reward = -0.143000 .\n",
      "Episode : 218 total reward = 0.190000 .\n",
      "Episode : 219 total reward = 0.093000 .\n",
      "Episode : 220 total reward = 0.024000 .\n",
      "Episode : 221 total reward = 0.392000 .\n",
      "Episode : 222 total reward = 0.125000 .\n",
      "Episode : 223 total reward = 0.355000 .\n",
      "Episode : 224 total reward = 0.285000 .\n",
      "Episode : 225 total reward = 0.496000 .\n",
      "Episode : 226 total reward = 0.228000 .\n",
      "Episode : 227 total reward = 0.151000 .\n",
      "Episode : 228 total reward = 0.586000 .\n",
      "Episode : 229 total reward = -0.198000 .\n",
      "Episode : 230 total reward = -0.034000 .\n",
      "Episode : 231 total reward = 0.449000 .\n",
      "Episode : 232 total reward = 0.120000 .\n",
      "Episode : 233 total reward = 0.362000 .\n",
      "Episode : 234 total reward = 0.399000 .\n",
      "Episode : 235 total reward = 0.236000 .\n",
      "Episode : 236 total reward = 0.147000 .\n",
      "Episode : 237 total reward = 0.339000 .\n",
      "Episode : 238 total reward = 0.235000 .\n",
      "Episode : 239 total reward = 0.556000 .\n",
      "Episode : 240 total reward = 0.321000 .\n",
      "Episode : 241 total reward = 0.081000 .\n",
      "Episode : 242 total reward = -0.225000 .\n",
      "Episode : 243 total reward = -0.027000 .\n",
      "Episode : 244 total reward = 0.558000 .\n",
      "Episode : 245 total reward = -0.183000 .\n",
      "Episode : 246 total reward = 0.559000 .\n",
      "Episode : 247 total reward = 0.390000 .\n",
      "Episode : 248 total reward = 0.473000 .\n",
      "Episode : 249 total reward = 0.454000 .\n"
     ]
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "ylim((-2, 1))\n",
    "env = gym.make('maze-sample-10x10-v0')\n",
    "\n",
    "# State 의 boundary\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "print(STATE_BOUNDS)\n",
    "# Maze의 size (10, 10)\n",
    "NUM_GRID = tuple((env.observation_space.high + np.ones(env.observation_space.shape)).astype(int))\n",
    "# action space\n",
    "ACTION = ['up', 'dw', 'ri', 'le']\n",
    "\n",
    "# gui환경의 Render 여부\n",
    "RENDER = False\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.2\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_table = defaultdict(lambda : [0.0, 0.0, 0.0, 0.0])\n",
    "    \n",
    "    # 학습하는 정책(벨만최적방정식) : 큐함수 업데이트 \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        q_1 = self.q_table[state][action]\n",
    "        q_2 = reward + self.discount_factor * max(self.q_table[next_state])\n",
    "        self.q_table[state][action] += self.learning_rate * (q_2 - q_1)\n",
    "        \n",
    "    # 입실론 탐욕 정책에 따라 행동을 반환 : sampling\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            # 무작위 행동 반환\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # 큐함수에 따른 행동 반환\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return int(action)\n",
    "    \n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "    \n",
    "    \n",
    "    # greedy 정책을 출력\n",
    "    def print_policy(self):\n",
    "        for y in range(NUM_GRID[0]):\n",
    "            for x in range(NUM_GRID[1]):\n",
    "                print(\"%s\"%ACTION[self.arg_max(self.q_table[str((x, y))])], end=\" \")\n",
    "                \n",
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_GRID[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_GRID[i] - 1) * STATE_BOUNDS[i][0] / bound_width\n",
    "            scaling = (NUM_GRID[i] - 1) / bound_width\n",
    "            bucket_index = int(round(scaling * state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)\n",
    "\n",
    "\n",
    "# 모형 학습\n",
    "if __name__ == \"__main__\":\n",
    "    env.reset()\n",
    "    agent = QLearningAgent(actions=list(range(env.action_space.n)))\n",
    "    scores = []\n",
    "    episodes = []\n",
    "    \n",
    "    for episode in range(250):\n",
    "        state = env.reset()\n",
    "        state = state_to_bucket(state)\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "                \n",
    "            action = agent.get_action(str(state))\n",
    "            \n",
    "            # 행동을 수행하고 다음 상태, 보상, 에피소드의 종료여부를 받아옴\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = state_to_bucket(next_state)\n",
    "            \n",
    "            # 큐함수를 업데이트\n",
    "            agent.learn(str(state), action, reward, str(next_state))\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # 모든 큐함수를 화면에 표시\n",
    "            #\n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode : %d total reward = %f .\"%(episode, total_reward))\n",
    "                episodes.append(episode)\n",
    "                scores.append(total_reward)\n",
    "                \n",
    "                if episode % 50 == 0:\n",
    "                    if not os.path.isdir('./save_graph'):\n",
    "                        os.mkdir('./save_graph')\n",
    "                    plt.plot(episodes, scores)\n",
    "                    plt.savefig('./save_graph/q_learning_basic.png')\n",
    "                break\n",
    "                \n",
    "            if np.mean(scores[-min(10, len(scores)):]) > 0.93:\n",
    "                RENDER = True\n",
    "                agent.print_policy()\n",
    "            else:\n",
    "                RENDER = False\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
