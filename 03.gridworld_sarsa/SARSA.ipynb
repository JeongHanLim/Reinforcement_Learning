{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T14:48:10.440058Z",
     "start_time": "2018-05-05T14:48:10.421545Z"
    }
   },
   "source": [
    "# SARSA\n",
    "- Sampling\n",
    "- On-Policy\n",
    "- 정책 평가(학습) : TD-Learning(큐함수에 대한 벨만기대방정식)\n",
    "    \n",
    "$$q(s, a) \\leftarrow q(s, a) + \\alpha(r + \\gamma q(s', a') - q(s, a))$$<br>\n",
    "\n",
    "- 정책 발전(행동선택) : $\\epsilon$ - greedy\n",
    "$$\\pi(s) = \\begin{cases} \n",
    "    a^* = argmax_{a \\in A} q(s, a), 1 - \\epsilon \\\\\n",
    "    \\text{random action}, \\ \\epsilon \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T14:48:10.440058Z",
     "start_time": "2018-05-05T14:48:10.421545Z"
    }
   },
   "source": [
    "## 기억할 점\n",
    "- TD(Temporal-difference) : online learning\n",
    "- Bootstrap : 자신의 상태를 통해 자기 자신을 업데이트\n",
    "- TD(0) 사용, TD(무한) -> MC\n",
    "    - n-step에서 n이 증가하면 **Bias는 감소, Variance는 증가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T05:15:17.208474Z",
     "start_time": "2018-05-08T05:15:17.144461Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T05:15:17.589415Z",
     "start_time": "2018-05-08T05:15:17.520417Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sarsa_Agent:\n",
    "    def __init__(self, actions):\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        \n",
    "        # defaultdict는 초기화 되어있지 않은 key값이 들어오면 미리 선언해준 값 대로 초기화\n",
    "        self.q_table = defaultdict(lambda : [0.0, 0.0, 0.0, 0.0])\n",
    "        \n",
    "    # 정책평가(evaluation)\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        current_q = self.q_table[state][action]\n",
    "        next_state_q = self.q_table[next_state][next_action]\n",
    "        new_q = (current_q + self.learning_rate * \n",
    "                 (reward + self.discount_factor * next_state_q - current_q))\n",
    "        self.q_table[state][action] = new_q\n",
    "        \n",
    "    # 정책발전(improvement : 엡실론 그리디)\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # 무작위 행동 변환\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # 큐함수에 따른 행동 변환\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "    \n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T05:15:19.310495Z",
     "start_time": "2018-05-08T05:15:19.308399Z"
    }
   },
   "outputs": [],
   "source": [
    "EPISODES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T05:31:48.382652Z",
     "start_time": "2018-05-08T05:15:19.752917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :  0  y :  0 [2.7912842919680606e-10, 0.0, 0.0, 3.0123560069871326e-06]\n",
      "x :  1  y :  0 [0.0, 2.4862551949245376e-10, 8.17496419217527e-09, 0.00013283272885585064]\n",
      "x :  2  y :  0 [-0.01791, -2.9701, 0.0, 0.004100423772283158]\n",
      "x :  3  y :  0 [0.0, 0.08718789530875552, -0.009000000000000001, 0.0]\n",
      "x :  4  y :  0 [0.0, 7.290000000000002e-07, 0.0, 0.0]\n",
      "\n",
      "x :  0  y :  1 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  1  y :  1 [2.1286241743220398e-07, -1.0, 0.0, -1.0]\n",
      "x :  2  y :  1 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y :  1 [0.0, 1.7105314189877845, -1.0, 1.5528003518712602e-05]\n",
      "x :  4  y :  1 [0.0, 0.0, 0.014353090320814582, 0.0]\n",
      "\n",
      "x :  0  y :  2 [0.0, 0.0, 0.0, -1.0]\n",
      "x :  1  y :  2 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  2  y :  2 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y :  2 [0.0, 0.0, 19.8369410460954, 0.0]\n",
      "x :  4  y :  2 [0.0, 0.0, 0.11812876850791953, 0.0]\n",
      "\n",
      "x :  0  y :  3 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  1  y :  3 [-1.0, 0.0, 0.0, 0.0]\n",
      "x :  2  y :  3 [1.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y :  3 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  4  y :  3 [0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "x :  0  y :  4 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  1  y :  4 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  2  y :  4 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  3  y :  4 [0.0, 0.0, 0.0, 0.0]\n",
      "x :  4  y :  4 [0.0, 0.0, 0.0, 0.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SARSA : Online Learning\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = Env()\n",
    "    sarsa_agent = Sarsa_Agent(actions=list(range(env.n_actions)))\n",
    "    \n",
    "    for episode in range(EPISODES):\n",
    "        # 게임 환경과 상태를 초기화\n",
    "        state = env.reset()\n",
    "        \n",
    "        # 현재 상태에 대한 행동을 선택(에피소드 마다 exploration)\n",
    "        action = sarsa_agent.get_action(str(state))\n",
    "        while True:\n",
    "            env.render()\n",
    "            \n",
    "            # 행동으로 reward, next_state, 에피소드의 종료 여부를 받음\n",
    "            next_state, reward, done = env.step(action) # step설정\n",
    "            \n",
    "            # next_state에서 next_action을 뽑아옴\n",
    "            next_action = sarsa_agent.get_action(str(next_state))\n",
    "            \n",
    "            # <s, a, r, s', a'>로 큐함수 업데이트\n",
    "            sarsa_agent.learn(str(state), action, reward, str(next_state), next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            # 모든 큐함수를 화면에 표시\n",
    "            env.print_value_all(sarsa_agent.q_table)\n",
    "            if done: # 종료가 되었다면~~~\n",
    "                if episode == 30:\n",
    "                    for y in range(5):\n",
    "                        for x in range(5):\n",
    "                            print(\"x : \", x, \" y : \", y, sarsa_agent.q_table[str([x, y])])\n",
    "                        print(\"\")\n",
    "                    time.sleep(100)\n",
    "                break\n",
    "    env.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T14:48:10.440058Z",
     "start_time": "2018-05-05T14:48:10.421545Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL)",
   "language": "python",
   "name": "reinforcement_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
